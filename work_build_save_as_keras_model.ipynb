{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efac9ef9-fabb-4120-9fd6-c827a59ad7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, time, scipy.io\n",
    "import pathlib\n",
    "import rawpy\n",
    "from IPython.display import Image\n",
    "import glob\n",
    "import PIL\n",
    "import PIL.Image as plim\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e48c233-e37d-40d4-bcb9-9d845349b304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.data.ops.dataset_ops.DatasetV2.prefetch(self, buffer_size, name=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "tf.data.Dataset.cache\n",
    "tf.data.Dataset.prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78500611-cd08-45ff-96b1-33c17efd3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "_errstr = \"Mode is unknown or incompatible with input array shape.\"\n",
    "\n",
    "\n",
    "def bytescale(data, cmin=None, cmax=None, high=255, low=0):\n",
    "    \"\"\"\n",
    "    Byte scales an array (image).\n",
    "    Byte scaling means converting the input image to uint8 dtype and scaling\n",
    "    the range to ``(low, high)`` (default 0-255).\n",
    "    If the input image already has dtype uint8, no scaling is done.\n",
    "    This function is only available if Python Imaging Library (PIL) is installed.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        PIL image data array.\n",
    "    cmin : scalar, optional\n",
    "        Bias scaling of small values. Default is ``data.min()``.\n",
    "    cmax : scalar, optional\n",
    "        Bias scaling of large values. Default is ``data.max()``.\n",
    "    high : scalar, optional\n",
    "        Scale max value to `high`.  Default is 255.\n",
    "    low : scalar, optional\n",
    "        Scale min value to `low`.  Default is 0.\n",
    "    Returns\n",
    "    -------\n",
    "    img_array : uint8 ndarray\n",
    "        The byte-scaled array.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from scipy.misc import bytescale\n",
    "    >>> img = np.array([[ 91.06794177,   3.39058326,  84.4221549 ],\n",
    "    ...                 [ 73.88003259,  80.91433048,   4.88878881],\n",
    "    ...                 [ 51.53875334,  34.45808177,  27.5873488 ]])\n",
    "    >>> bytescale(img)\n",
    "    array([[255,   0, 236],\n",
    "           [205, 225,   4],\n",
    "           [140,  90,  70]], dtype=uint8)\n",
    "    >>> bytescale(img, high=200, low=100)\n",
    "    array([[200, 100, 192],\n",
    "           [180, 188, 102],\n",
    "           [155, 135, 128]], dtype=uint8)\n",
    "    >>> bytescale(img, cmin=0, cmax=255)\n",
    "    array([[91,  3, 84],\n",
    "           [74, 81,  5],\n",
    "           [52, 34, 28]], dtype=uint8)\n",
    "    \"\"\"\n",
    "    if data.dtype == np.uint8:\n",
    "        return data\n",
    "\n",
    "    if high > 255:\n",
    "        raise ValueError(\"`high` should be less than or equal to 255.\")\n",
    "    if low < 0:\n",
    "        raise ValueError(\"`low` should be greater than or equal to 0.\")\n",
    "    if high < low:\n",
    "        raise ValueError(\"`high` should be greater than or equal to `low`.\")\n",
    "\n",
    "    if cmin is None:\n",
    "        cmin = data.min()\n",
    "    if cmax is None:\n",
    "        cmax = data.max()\n",
    "\n",
    "    cscale = cmax - cmin\n",
    "    if cscale < 0:\n",
    "        raise ValueError(\"`cmax` should be larger than `cmin`.\")\n",
    "    elif cscale == 0:\n",
    "        cscale = 1\n",
    "\n",
    "    scale = float(high - low) / cscale\n",
    "    bytedata = (data - cmin) * scale + low\n",
    "    return (bytedata.clip(low, high) + 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "def toimage(arr, high=255, low=0, cmin=None, cmax=None, pal=None,\n",
    "            mode=None, channel_axis=None):\n",
    "    \"\"\"Takes a numpy array and returns a PIL image.\n",
    "    This function is only available if Python Imaging Library (PIL) is installed.\n",
    "    The mode of the PIL image depends on the array shape and the `pal` and\n",
    "    `mode` keywords.\n",
    "    For 2-D arrays, if `pal` is a valid (N,3) byte-array giving the RGB values\n",
    "    (from 0 to 255) then ``mode='P'``, otherwise ``mode='L'``, unless mode\n",
    "    is given as 'F' or 'I' in which case a float and/or integer array is made.\n",
    "    .. warning::\n",
    "        This function uses `bytescale` under the hood to rescale images to use\n",
    "        the full (0, 255) range if ``mode`` is one of ``None, 'L', 'P', 'l'``.\n",
    "        It will also cast data for 2-D images to ``uint32`` for ``mode=None``\n",
    "        (which is the default).\n",
    "    Notes\n",
    "    -----\n",
    "    For 3-D arrays, the `channel_axis` argument tells which dimension of the\n",
    "    array holds the channel data.\n",
    "    For 3-D arrays if one of the dimensions is 3, the mode is 'RGB'\n",
    "    by default or 'YCbCr' if selected.\n",
    "    The numpy array must be either 2 dimensional or 3 dimensional.\n",
    "    \"\"\"\n",
    "    data = np.asarray(arr)\n",
    "    if np.iscomplexobj(data):\n",
    "        raise ValueError(\"Cannot convert a complex-valued array.\")\n",
    "    shape = list(data.shape)\n",
    "    valid = len(shape) == 2 or ((len(shape) == 3) and\n",
    "                                ((3 in shape) or (4 in shape)))\n",
    "    if not valid:\n",
    "        raise ValueError(\"'arr' does not have a suitable array shape for \"\n",
    "                         \"any mode.\")\n",
    "    if len(shape) == 2:\n",
    "        shape = (shape[1], shape[0])  # columns show up first\n",
    "        if mode == 'F':\n",
    "            data32 = data.astype(np.float32)\n",
    "            image = Image.frombytes(mode, shape, data32.tostring())\n",
    "            return image\n",
    "        if mode in [None, 'L', 'P']:\n",
    "            bytedata = bytescale(data, high=high, low=low,\n",
    "                                 cmin=cmin, cmax=cmax)\n",
    "            image = Image.frombytes('L', shape, bytedata.tostring())\n",
    "            if pal is not None:\n",
    "                image.putpalette(np.asarray(pal, dtype=np.uint8).tostring())\n",
    "                # Becomes a mode='P' automagically.\n",
    "            elif mode == 'P':  # default gray-scale\n",
    "                pal = (np.arange(0, 256, 1, dtype=np.uint8)[:, np.newaxis] *\n",
    "                       np.ones((3,), dtype=np.uint8)[np.newaxis, :])\n",
    "                image.putpalette(np.asarray(pal, dtype=np.uint8).tostring())\n",
    "            return image\n",
    "        if mode == '1':  # high input gives threshold for 1\n",
    "            bytedata = (data > high)\n",
    "            image = Image.frombytes('1', shape, bytedata.tostring())\n",
    "            return image\n",
    "        if cmin is None:\n",
    "            cmin = np.amin(np.ravel(data))\n",
    "        if cmax is None:\n",
    "            cmax = np.amax(np.ravel(data))\n",
    "        data = (data*1.0 - cmin)*(high - low)/(cmax - cmin) + low\n",
    "        if mode == 'I':\n",
    "            data32 = data.astype(np.uint32)\n",
    "            image = Image.frombytes(mode, shape, data32.tostring())\n",
    "        else:\n",
    "            raise ValueError(_errstr)\n",
    "        return image\n",
    "\n",
    "    # if here then 3-d array with a 3 or a 4 in the shape length.\n",
    "    # Check for 3 in datacube shape --- 'RGB' or 'YCbCr'\n",
    "    if channel_axis is None:\n",
    "        if (3 in shape):\n",
    "            ca = np.flatnonzero(np.asarray(shape) == 3)[0]\n",
    "        else:\n",
    "            ca = np.flatnonzero(np.asarray(shape) == 4)\n",
    "            if len(ca):\n",
    "                ca = ca[0]\n",
    "            else:\n",
    "                raise ValueError(\"Could not find channel dimension.\")\n",
    "    else:\n",
    "        ca = channel_axis\n",
    "\n",
    "    numch = shape[ca]\n",
    "    if numch not in [3, 4]:\n",
    "        raise ValueError(\"Channel axis dimension is not valid.\")\n",
    "\n",
    "    bytedata = bytescale(data, high=high, low=low, cmin=cmin, cmax=cmax)\n",
    "    if ca == 2:\n",
    "        strdata = bytedata.tostring()\n",
    "        shape = (shape[1], shape[0])\n",
    "    elif ca == 1:\n",
    "        strdata = np.transpose(bytedata, (0, 2, 1)).tostring()\n",
    "        shape = (shape[2], shape[0])\n",
    "    elif ca == 0:\n",
    "        strdata = np.transpose(bytedata, (1, 2, 0)).tostring()\n",
    "        shape = (shape[2], shape[1])\n",
    "    if mode is None:\n",
    "        if numch == 3:\n",
    "            mode = 'RGB'\n",
    "        else:\n",
    "            mode = 'RGBA'\n",
    "\n",
    "    if mode not in ['RGB', 'RGBA', 'YCbCr', 'CMYK']:\n",
    "        raise ValueError(_errstr)\n",
    "\n",
    "    if mode in ['RGB', 'YCbCr']:\n",
    "        if numch != 3:\n",
    "            raise ValueError(\"Invalid array shape for mode.\")\n",
    "    if mode in ['RGBA', 'CMYK']:\n",
    "        if numch != 4:\n",
    "            raise ValueError(\"Invalid array shape for mode.\")\n",
    "\n",
    "    # Here we know data and mode is correct\n",
    "    image = Image.frombytes(mode, shape, strdata)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55893e3c-28d6-43d7-941e-7171da6814d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./result_Sony/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m result_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./result_Sony/keras_model_test/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m target_fns \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(gt_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0*.ARW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m target_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(target_fn)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m target_fn \u001b[38;5;129;01min\u001b[39;00m target_fns]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "input_dir = './Sony/short/'\n",
    "gt_dir = './Sony/long/'\n",
    "checkpoint_dir = './result_Sony/'\n",
    "result_dir = './result_Sony/keras_model_test/'\n",
    "\n",
    "target_fns = glob.glob(gt_dir + '0*.ARW')\n",
    "target_ids = [int(os.path.basename(target_fn)[0:5]) for target_fn in target_fns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff887f7-f6eb-4735-93f8-e77426847a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(x, n_filters):\n",
    "    x1 = tf.keras.layers.Conv2D(n_filters, [3,3], padding = 'SAME', activation = leaky_relu_layer, kernel_initializer = 'random_normal')(x)\n",
    "    x2 = tf.keras.layers.Conv2D(n_filters, [3,3], padding = 'SAME', activation = leaky_relu_layer, kernel_initializer = 'random_normal')(x1)\n",
    "    return x2\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "    f = double_conv(x, n_filters)\n",
    "    p = tf.keras.layers.MaxPool2D(2, padding = 'SAME')(f)\n",
    "    #  p = tf.keras.layers.Dropout(0.3)(p)\n",
    "    return f, p\n",
    "\n",
    "def upsample_block(x1, n_filters, x2 = None):\n",
    "    pool_size = 2 \n",
    "    # output_chanel = n_filter\n",
    "    input_channels = n_filters*2\n",
    "    \n",
    "    deconv_filter = tf.Variable(tf.random.truncated_normal([pool_size, pool_size, n_filters, input_channels], stddev = 0.02))\n",
    "    # print(deconv_filter)\n",
    "    # print('x2 down_conw',tf.shape(x2))\n",
    "    # print('x1 up_conw',tf.shape(x1))\n",
    "    x = tf.keras.layers.Conv2DTranspose(n_filters, pool_size, strides = (pool_size, pool_size),padding='same')(x1)\n",
    "    print(np.shape(x))\n",
    "    x = tf.concat([x, x2], 3)\n",
    "    x.set_shape([None, None, None, n_filters * 2])\n",
    "    \n",
    "    #  x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = double_conv(x, n_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet_model():\n",
    "    inputs = tf.keras.layers.Input(shape=(None,None,4))\n",
    "    \n",
    "    conv_1, p1 = downsample_block(inputs, 32)\n",
    "    conv_2, p2 = downsample_block(p1, 64)\n",
    "    conv_3, p3 = downsample_block(p2, 128)\n",
    "    conv_4, p4 = downsample_block(p3, 256)\n",
    "    \n",
    "    conv_5 = double_conv(p4, 512)\n",
    "    \n",
    "    conv_6 = upsample_block(conv_5, 256, conv_4)\n",
    "    conv_7 = upsample_block(conv_6, 128,  conv_3)\n",
    "    conv_8 = upsample_block(conv_7, 64, conv_2)\n",
    "    conv_9 = upsample_block(conv_8, 32, conv_1)\n",
    "    conv_10 = tf.keras.layers.Conv2D(12, 1, padding=\"same\", activation = None)(conv_9)\n",
    "    out = tf.nn.depth_to_space(conv_10, 2)\n",
    "    unet_model = tf.keras.Model(inputs, out, name=\"U-Net\")\n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a2b8b4-9619-4c72-bdf7-b96406355a5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m leaky_relu_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLeakyReLU(\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m      2\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n\u001b[0;32m      3\u001b[0m ps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "leaky_relu_layer = tf.keras.layers.LeakyReLU(0.2)\n",
    "learning_rate = 1e-4\n",
    "ps = 512\n",
    "save_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28878b97-26c5-4acd-ace0-103085828693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.conv2d_transpose), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(2, 2, 256, 512) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.conv2d_transpose_1), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(2, 2, 128, 256) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.conv2d_transpose_2), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(2, 2, 64, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.conv2d_transpose_3), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(2, 2, 32, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x24622a7ce20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_model = build_unet_model()\n",
    "learning_rate = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "unet_model.compile(optimizer, keras.losses.MeanAbsoluteError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b6b77b-2e9b-4028-8083-0b78c1268267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_raw(raw):\n",
    "  im = raw.raw_image_visible.astype(np.float32)\n",
    "  im = np.maximum(im - 512, 0)/(16383 - 512)\n",
    "  im = np.expand_dims(im, axis = 2)\n",
    "  # print(np.shape(im))\n",
    "  img_shape = im.shape\n",
    "  H = img_shape[0]\n",
    "  W = img_shape[1]\n",
    "\n",
    "  out = np.concatenate((im[0:H:2, 0:W:2, :],\n",
    "                          im[0:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 0:W:2, :]), axis=2)\n",
    "  # print(np.shape(out))\n",
    "  return out\n",
    "#разделяет raw на RGBG каналы\n",
    "def divide_layers(layer, channel):\n",
    "  only_one_layer = []\n",
    "  for i in range(len(layer)):\n",
    "    first_layer = []\n",
    "    for j in range(len(layer[0])):\n",
    "      first_layer.append(layer[i][j][channel])\n",
    "    only_one_layer.append(first_layer)\n",
    "  return only_one_layer\n",
    "\n",
    "def print_subplot(ax, channel, color_space, name):\n",
    "  ax.imshow(channel, cmap = color_space)\n",
    "  ax.set_title(name)\n",
    "def print_image(image):\n",
    "  image = np.array(image)\n",
    "  R, G_1, B, G= cv2.split(image)\n",
    "  fig, (ax_1, ax_2, ax_3, ax_4) = plt.subplots(1, 4, figsize = [20, 10])\n",
    "  print_subplot(ax_1, R, 'Reds', 'Red_channel')\n",
    "  print_subplot(ax_2, G_1, 'Greens', 'Green_channel')\n",
    "  print_subplot(ax_3, B, 'Blues', 'Blue_channel')\n",
    "  print_subplot(ax_4, G, 'Greens', 'Green_2_channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f419e8-5ea7-4ad2-8148-23f2daf79a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "def loss_1(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b989b9-e24a-418b-8665-c454f1e90c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_optimization(x,y):\n",
    "    with tf.GradientTape() as g:\n",
    "        prediction = unet_model(x)\n",
    "        loss = tf.reduce_mean(tf.abs(prediction - y))\n",
    "    trainable_variables = unet_model.trainable_variables\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0499e666-63f6-4a85-9961-612082f72796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, time: 212.1296s, mean loss 0.046\n",
      "epoch: 2, time: 99.4767s, mean loss 0.037\n",
      "epoch: 3, time: 83.1773s, mean loss 0.036\n",
      "epoch: 4, time: 87.1225s, mean loss 0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksi\\AppData\\Local\\Temp\\ipykernel_9696\\2496767527.py:160: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  strdata = bytedata.tostring()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Layer tf.nn.conv2d_transpose was passed non-JSON-serializable arguments. Arguments had types: {'filters': <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>, 'output_shape': [<class 'str'>, <class 'int'>, <class 'int'>], 'strides': [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]}. They cannot be serialized out when saving the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m     temp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((gt_patch[\u001b[38;5;241m0\u001b[39m, :, :, :], output[\u001b[38;5;241m0\u001b[39m, :, :, :]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     71\u001b[0m     toimage(temp \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, cmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, cmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\n\u001b[0;32m     72\u001b[0m         result_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%04d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%05d\u001b[39;00m\u001b[38;5;124m_00_train_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, target_id, ratio))\n\u001b[1;32m---> 73\u001b[0m     \u001b[43munet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheck_points/keras_model/kerasmodel\u001b[39;49m\u001b[38;5;132;43;01m%03d\u001b[39;49;00m\u001b[38;5;124;43m.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcnt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mst)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, mean loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(np\u001b[38;5;241m.\u001b[39mmean(loss_epoch))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\node.py:224\u001b[0m, in \u001b[0;36mNode.serialize\u001b[1;34m(self, make_node_key, node_conversion_map)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     kwarg_types \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28mtype\u001b[39m, kwargs)\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m was passed non-JSON-serializable arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments had types: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(kwarg_types)\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. They cannot be serialized out \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen saving the model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m     )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# `kwargs` is added to each Tensor in the first arg. This should be\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# changed in a future version of the serialization format.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserialize_first_arg_tensor\u001b[39m(t):\n",
      "\u001b[1;31mTypeError\u001b[0m: Layer tf.nn.conv2d_transpose was passed non-JSON-serializable arguments. Arguments had types: {'filters': <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>, 'output_shape': [<class 'str'>, <class 'int'>, <class 'int'>], 'strides': [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]}. They cannot be serialized out when saving the model."
     ]
    }
   ],
   "source": [
    "gt_images = [None] * 6000\n",
    "input_images = {}\n",
    "input_images['300'] = [None] * len(target_ids)\n",
    "input_images['250'] = [None] * len(target_ids)\n",
    "input_images['100'] = [None] * len(target_ids)\n",
    "cnt = 0\n",
    "\n",
    "for epoch in range(1+903, 2001):\n",
    "    if epoch > 1500:\n",
    "        learning_rate = 1e-5\n",
    "    st = time.time()\n",
    "    loss_epoch = []\n",
    "    st = time.time()\n",
    "    for ind in np.random.permutation(len(target_ids)):\n",
    "        target_id = target_ids[ind]\n",
    "        in_files = glob.glob(input_dir + '%05d_0?*.ARW' % target_id)\n",
    "        in_path = in_files[np.random.randint(0, len(in_files))]\n",
    "        in_fn = os.path.basename(in_path)\n",
    "\n",
    "        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % target_id)\n",
    "        gt_path = gt_files[0]\n",
    "        gt_fn = os.path.basename(gt_path)\n",
    "        in_exposure = float(in_fn[9:-5])\n",
    "        gt_exposure = float(gt_fn[9:-5])\n",
    "        ratio = min(gt_exposure / in_exposure, 300)\n",
    "\n",
    "        if input_images[str(ratio)[0:3]][ind] is None:\n",
    "            raw = rawpy.imread(in_path)\n",
    "            input_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n",
    "\n",
    "            gt_raw = rawpy.imread(gt_path)\n",
    "            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n",
    "        \n",
    "        H = input_images[str(ratio)[0:3]][ind].shape[1]\n",
    "        W = input_images[str(ratio)[0:3]][ind].shape[2]\n",
    "        xx = np.random.randint(0, W - ps)\n",
    "        yy = np.random.randint(0, H - ps)\n",
    "        input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n",
    "        gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n",
    "        \n",
    "        if np.random.randint(2, size=1)[0] == 1:  # random flip\n",
    "            input_patch = np.flip(input_patch, axis=1)\n",
    "            gt_patch = np.flip(gt_patch, axis=1)\n",
    "        if np.random.randint(2, size=1)[0] == 1:\n",
    "            input_patch = np.flip(input_patch, axis=2)\n",
    "            gt_patch = np.flip(gt_patch, axis=2)\n",
    "        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n",
    "            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
    "            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n",
    "        # print(\"Min and max pixel values:\",np.min(input_patch), np.max(input_patch))\n",
    "        input_patch = np.minimum(input_patch, 1.0)\n",
    "        loss = run_optimization(input_patch, gt_patch)\n",
    "        loss_epoch.append(loss)\n",
    "        if epoch % 100 == 0:\n",
    "            output = unet_model(input_patch)  \n",
    "            output = np.minimum(np.maximum(output, 0), 1)\n",
    "            if not os.path.isdir(result_dir + '%04d' % epoch):\n",
    "                os.makedirs(result_dir + '%04d' % epoch)\n",
    "            temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :, :]), axis=1)\n",
    "            toimage(temp * 255, high=255, low=0, cmin=0, cmax=255, mode = 'RGB').save(\n",
    "                result_dir + '%04d/%05d_00_train_%d.png' % (epoch, target_id, ratio))\n",
    "\n",
    "    if epoch % save_freq == 0 or epoch == 5:\n",
    "        cnt +=1\n",
    "        output = unet_model(input_patch)  \n",
    "        output = np.minimum(np.maximum(output, 0), 1)\n",
    "        if not os.path.isdir(result_dir + '%04d' % epoch):\n",
    "            os.makedirs(result_dir + '%04d' % epoch)\n",
    "        temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :, :]), axis=1)\n",
    "        toimage(temp * 255, high=255, low=0, cmin=0, cmax=255, mode = 'RGB').save(\n",
    "            result_dir + '%04d/%05d_00_train_%d.png' % (epoch, target_id, ratio))\n",
    "        unet_model.save(filepath=result_dir + \"check_points/keras_model/kerasmodel%03d.keras\" % cnt)\n",
    "    print(f'epoch: {epoch}, time: { \"{:.4f}\".format(time.time() - st)}s, mean loss {\"{:.3f}\".format(np.mean(loss_epoch))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9804e9f-768a-4ecf-95da-e88ffc5e7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = rawpy.imread(f'{target_fns[14]}')\n",
    "raw = pack_raw(im)\n",
    "crop = tf.image.stateless_random_crop(\n",
    "      raw, size=[ps, ps, 4], seed=(3,8))\n",
    "test = np.expand_dims(crop, axis=0)\n",
    "out_im = unet_model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb2cb5-f61c-4db8-8a14-5fe5f6f9c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_im = np.minimum(np.maximum(out_im, 0), 1)\n",
    "plt.imshow(out_im[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00907ace-4b64-4673-9764-60f4b7997541",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fn = glob.glob('./Sony/' + '*.ARW')\n",
    "print(test_fn)\n",
    "im = rawpy.imread(f'{test_fn[0]}')\n",
    "raw = pack_raw(im)\n",
    "for i in range(4):\n",
    "    for j in range(6): \n",
    "        crop = raw[512*i:512*(i+1), 512*j:512*(j+1), :]\n",
    "        test = np.expand_dims(crop, axis=0) * 50\n",
    "        out_im = unet_model(test)\n",
    "        out_im = np.minimum(np.maximum(out_im, 0), 1)\n",
    "        toimage(out_im[0] * 255, high=255, low=0, cmin=0, cmax=255, mode = 'RGB').save(\n",
    "                    result_dir + \"test_my_data/\" + f'{int(os.path.basename(test_fn[0])[4:8])}_{i}{j}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
